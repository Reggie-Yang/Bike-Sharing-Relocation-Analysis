{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read datasets\n",
    "hour = pd.read_csv('/Users/reggieyang/Downloads/Bike-Sharing-Dataset/hour.csv')\n",
    "day = pd.read_csv('/Users/reggieyang/Downloads/Bike-Sharing-Dataset/day.csv')\n",
    "bike2011 = pd.read_csv('/Users/reggieyang/Downloads/2011-capitalbikeshare-tripdata.csv')\n",
    "weather1 = pd.read_html('https://i-weather.com/weather/washington/history/daily-history/?gid=4140963&date=2017-01-01&station=19064&language=english&country=us-united-states')[6]\n",
    "weather2 = pd.read_html('https://i-weather.com/weather/washington/history/daily-history/?gid=4140963&date=2017-01-02&station=19064&language=english&country=us-united-states')[6]\n",
    "weather3 = pd.read_html('https://i-weather.com/weather/washington/history/daily-history/?gid=4140963&date=2017-01-03&station=19064&language=english&country=us-united-states')[6]\n",
    "weather1['year']=datetime.date(2017,1,1).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extract hour weather data\n",
    "begin = datetime.date(2017,1,1)\n",
    "end = datetime.date(2017,12,31)\n",
    " \n",
    "d = begin\n",
    "delta = datetime.timedelta(days=1)\n",
    "year_weather = pd.DataFrame()\n",
    "while d <= end:\n",
    "    weather = pd.read_html('https://i-weather.com/weather/washington/history/daily-history/?gid=4140963&date={}&station=19064&language=english&country=us-united-states'.format(d.strftime(\"%Y-%m-%d\")))[6]\n",
    "    weather['Date']=d.strftime(\"%Y-%m-%d\")\n",
    "    year_weather = year_weather.append(weather)\n",
    "    d += delta\n",
    "\n",
    "year_weather.reset_index(level=0,drop=True,inplace=True)\n",
    "year_weather.drop('Icon',axis=1,inplace=True)\n",
    "year_weather.drop('Wind Gust',axis=1,inplace=True)\n",
    "hour_weather = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/DC Weather /year_hour_weather.csv')\n",
    "hour_weather.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "##group  different variables of weather\n",
    "hour_weather['hour']=hour_weather['Time'].str.extract('(\\d\\d):\\d\\d')\n",
    "hour_weather['Wind_km/h']=hour_weather['Wind'].str.extract('\\d°(\\d*)\\sKm/h')\n",
    "hour_weather['Wind_km/h'] = hour_weather['Wind_km/h'].fillna(0).astype('int64')\n",
    "hour_weather['Humidity_percentage'] = hour_weather['Rel. humidity'].str.extract('(\\d+)%').astype('int64')\n",
    "hour_weather['Temperature_c'] = hour_weather['Temperature'].str.extract('(\\d+)°C').astype('int64')\n",
    "hour_weather['Relative Temperature_c'] = hour_weather['Relative Temperature'].str.extract('(\\d+)°C').astype('int64')\n",
    "hour_weather['weather'] = hour_weather['DescriptionDetails'].str.extract(\"GetShortDescription\\((\\d+),\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build weather dictionary into dataset\n",
    "weather_dict = {'1':'Clear','2':'Partly cloudy','3':'Few clouds','4':'Cloudy','7':'Rain','10':'Thunder','26':'Snow'}\n",
    "hour_weather['weather'] = hour_weather['weather'].map(weather_dict)\n",
    "##drop other unrelative variables\n",
    "hour_weather.drop('Dew Point',axis=1,inplace=True)\n",
    "hour_weather.drop('Pressure',axis=1,inplace=True)\n",
    "hour_weather.drop('DescriptionDetails',axis=1,inplace=True)\n",
    "hour_weather.drop('Relative Temperature',axis=1,inplace=True)\n",
    "hour_weather.drop('Temperature',axis=1,inplace=True)\n",
    "hour_weather.drop('Wind',axis=1,inplace=True)\n",
    "hour_weather.drop('Time',axis=1,inplace=True)\n",
    "hour_weather.drop('Rel. humidity',axis=1,inplace=True)\n",
    "hour_weather.rename({'Date':'date'},axis='columns',inplace=True)\n",
    "hour_weather=hour_weather[['date','hour',    'Wind_km/h', 'Humidity_percentage',\n",
    "       'Temperature_c', 'Relative Temperature_c', 'weather']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Merge datasets, and delete duplications in the dataset\n",
    "hour_weather['merge']= hour_weather['date']+hour_weather['hour']\n",
    "\n",
    "hour_weather['merge'].nunique()\n",
    "hour_weather.drop_duplicates('merge',keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sharebike dataset in 2017\n",
    "bike2017 = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /capitalbikeshare2017.csv')\n",
    "bike2017.head()\n",
    "bike2017.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "#group by hour\n",
    "bike2017_practice = bike2017\n",
    "bike2017_practice = bike2017_practice[['Start date','Member type']]\n",
    "bike2017_practice['hour'] = bike2017_practice['Start date'].str.extract('(\\d{4}-\\d\\d-\\d\\d\\s\\d\\d):',expand=False)\n",
    "bike2017_practice = pd.get_dummies(bike2017_practice,columns=['Member type'])\n",
    "\n",
    "\n",
    "bike2017_practice.columns\n",
    "hour_count = bike2017_practice.groupby(\"hour\",as_index=False).agg('count')\n",
    "hour_count = hour_count['Start date']\n",
    "bike_hour = bike2017_practice.groupby(\"hour\",as_index=False).agg({'Member type_Casual':np.sum,'Member type_Member':np.sum})\n",
    "bike_hour['cnt'] = hour_count\n",
    "bike_hour.columns\n",
    "bike_hour.head()\n",
    "bike_hour['date_bike']=bike_hour['hour'].str.extract('(\\d{4}-\\d\\d-\\d\\d)',expand=False)\n",
    "bike_hour['hour_bike']=bike_hour['hour'].str.extract('\\s(\\d\\d)',expand=False)\n",
    "bike_hour=bike_hour[['date_bike','hour_bike', 'Member type_Casual', 'Member type_Member', 'cnt' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##merge data\n",
    "bike_hour['merge'] = bike_hour['date_bike']+bike_hour['hour_bike']\n",
    "\n",
    "\n",
    "bike_weather = bike_hour.merge(hour_weather,on='merge',how = 'inner')\n",
    "\n",
    "bike_weather.columns\n",
    "\n",
    "bike_weather.isnull().sum()\n",
    "\n",
    "bike_weather.drop('merge',axis=1,inplace=True)\n",
    "bike_weather.drop('date',axis=1,inplace=True)\n",
    "bike_weather.drop('hour',axis=1,inplace=True)\n",
    "bike_weather.rename({'date_bike':'date','hour_bike':'hour'},axis=1,inplace=True)\n",
    "\n",
    "bike_weather=bike_weather[['date', 'hour',\n",
    "       'Wind_km/h', 'Humidity_percentage', 'Temperature_c',\n",
    "       'Relative Temperature_c', 'weather', 'Member type_Casual', 'Member type_Member','cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##prepare dataset for descriptive analysis\n",
    "bike = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/bike hour/bike_hour.csv',parse_dates=['date'])\n",
    "bike.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "bike.shape\n",
    "bike.head()\n",
    "bike.info()\n",
    "##add seasons\n",
    "bike['Month'] = bike['date'].astype('str').apply(lambda x:x.split('-')[1])\n",
    "def add_season(df):\n",
    "    if df=='03' or df=='04' or df=='05':\n",
    "        return 1\n",
    "    if df=='06' or df=='07' or df=='08':\n",
    "        return 2\n",
    "    if df=='09' or df=='10' or df=='11':\n",
    "        return 3\n",
    "    if df=='12' or df=='01' or df=='02':\n",
    "        return 4\n",
    "bike['Season'] = bike['Month'].apply(add_season)\n",
    "##add holiday\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "usb = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "not_holiday = pd.date_range(start='2017-01-01',end='2017-12-31',freq=usb)\n",
    "def extract_holiday(df):\n",
    "    if df in not_holiday:\n",
    "        return 0\n",
    "    if df not in not_holiday:\n",
    "        return 1\n",
    "    \n",
    "bike['Weekend_and_holiday']=bike['date'].apply(extract_holiday)\n",
    "##add weekend\n",
    "not_weekend = pd.date_range(start='2017-01-01',end='2017-12-31',freq='B')\n",
    "def extract_week(df):\n",
    "    if df in not_weekend:\n",
    "        return 0\n",
    "    if df not in not_weekend:\n",
    "        return 1\n",
    "bike['weekend']=bike['date'].apply(extract_week)\n",
    "##change column order\n",
    "bike.columns\n",
    "bike.drop('Month',axis=1,inplace=True)\n",
    "bike = bike[['date', 'hour','weekend','Weekend_and_holiday',  'Season','Wind_km/h', 'Humidity_percentage', 'Temperature_c',\n",
    "       'Relative Temperature_c', 'weather', 'Member type_Casual',\n",
    "       'Member type_Member', 'cnt']]\n",
    "bike.rename({'cnt':'Demand'},inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##devide datetime into seprate columns\n",
    "bike = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/bike hour/2017_CapitalBike.csv')\n",
    "bike.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "bike.head()\n",
    "bike['date'] = bike['date'].astype('str')\n",
    "bike['Year'] = bike['date'].apply(lambda x:x.split('-')[0])\n",
    "bike['Month'] = bike['date'].apply(lambda x:x.split('-')[1])\n",
    "bike['Day'] = bike['date'].apply(lambda x:x.split('-')[2])\n",
    "\n",
    "##convert weather column\n",
    "bike['date'] = bike['date'].astype('str')\n",
    "\n",
    "\n",
    "bike.drop('date',axis=1,inplace=True)\n",
    "bike.columns\n",
    "bike = bike[['Year','Month','Day','hour', 'weekend', 'Weekend_and_holiday', 'Season', 'weather','Wind_km/h',\n",
    "       'Humidity_percentage', 'Temperature_c', 'Relative Temperature_c',\n",
    "        'Member type_Casual', 'Member type_Member', 'Demand'\n",
    "        ]]\n",
    "##weather types\n",
    "weather_dict = {'Clear':'Clear','Partly cloudy':'Cloudy','Few clouds':'Cloudy','Cloudy':'Cloudy','Rain':'Rain','Thunder':'Thunder','Snow':'Snow'}\n",
    "bike['weather'] = bike['weather'].map(weather_dict)\n",
    "bike['weather'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get stations' location info\n",
    "df2 = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /capitalbikeshare2017.csv')\n",
    "df2.columns\n",
    "df2.head()\n",
    "df2['Start station number'].nunique()\n",
    "df2['End station number'].nunique()\n",
    "df2['Bike number'].nunique()\n",
    "\n",
    "bike_station = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /Capital_Bike_Share_Locations (1).csv')\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "location = geolocator.reverse(\"38.92624, -77.03705\")\n",
    "location.address.split(',')[-2]\n",
    "\n",
    "\n",
    "bike_station = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /Capital_Bike_Share_Locations (1).csv')\n",
    "bike_station.head()\n",
    "##get paired longitude and latitude info of stations\n",
    "cor = tuple(zip(list(bike_station['LATITUDE']),list(bike_station['LONGITUDE'])))\n",
    "cor_array = []\n",
    "\n",
    "##get zip codes\n",
    "for i in cor:\n",
    "    \n",
    "    reverse_geocode_result = gmaps.reverse_geocode((i[0],i[1]))\n",
    "    if reverse_geocode_result[0]['address_components'][-1]['types'][0]=='postal_code':\n",
    "        cor_array.append(reverse_geocode_result[0]['address_components'][-1]['long_name'])\n",
    "    elif reverse_geocode_result[0]['address_components'][-2]['types'][0]=='postal_code':\n",
    "        cor_array.append(reverse_geocode_result[0]['address_components'][-2]['long_name'])\n",
    "    elif reverse_geocode_result[0]['address_components'][-3]['types'][0]=='postal_code':\n",
    "        cor_array.append(reverse_geocode_result[0]['address_components'][-3]['long_name'])\n",
    "    else:\n",
    "        cor_array.append(np.nan)\n",
    "        \n",
    "cor_array\n",
    "\n",
    "zip_code = np.array(cor_array)\n",
    "bike_station['ZIP_CODE'] = zip_code\n",
    "bike_station['ZIP_CODE'].nunique()\n",
    "\n",
    "location = geolocator.reverse(cor[3][0],cor[3][1])\n",
    "print(location.address.split(',')[-2])\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "location = geolocator.geocode(bike_station['ADDRESS'][0])\n",
    "print(location)\n",
    "gmaps = googlemaps.Client(key='AIzaSyCJR0f7wBGN8KPy8rDiK0gnG3NL9dZGt4M')\n",
    "reverse_geocode_result2 = gmaps.reverse_geocode((cor[1][0], cor[1][1]))\n",
    "reverse_geocode_result2[0]['address_components'][-1]['long_name']\n",
    "reverse_geocode_result = gmaps.reverse_geocode((cor[0][0], cor[0][1]))\n",
    "reverse_geocode_result[0]['address_components'][-1]['types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------balance & unbalance analysis--------------\n",
    "bike_2017 = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /capitalbikeshare2017(original dataset).csv')\n",
    "bike_2017.head()\n",
    "del bike_2017['Unnamed: 0']\n",
    "##---bike station dictionary\n",
    "##prepare the dataset\n",
    "bike_station = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /Capital_Bike_Share_Locations (1).csv')\n",
    "bike_station.head()\n",
    "bike_station['SLOTS'] = bike_station['NUMBER_OF_BIKES']+bike_station['NUMBER_OF_EMPTY_DOCKS']\n",
    "bike_station = bike_station[['TERMINAL_NUMBER','SLOTS']]\n",
    "bike_station.set_index('TERMINAL_NUMBER',inplace=True)\n",
    "\n",
    "bike_station_dict = bike_station.to_dict()\n",
    "bike_station_dict.keys()\n",
    "bike_2017['Start date'] = pd.to_datetime(bike_2017['Start date'])\n",
    "bike_2017['End date'] = pd.to_datetime(bike_2017['End date'])\n",
    "bike_2017['End date'].resample('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----hour bike demand\n",
    "bike_h_demand = bike_2017[['Start date','Start station number','Bike number']]\n",
    "bike_h_demand['Start date'] = bike_h_demand['Start date'].str.extract('(\\d{4}-\\d\\d-\\d\\d)\\s\\d\\d:\\d\\d:\\d\\d')\n",
    "\n",
    "bike_h_demand2 = bike_h_demand.groupby(['Start date','Start station number']).agg('count')\n",
    "bike_h_demand2.rename({'Bike number':'Demand'},axis=1,inplace=True)\n",
    "bike_h_demand2['index'] = bike_h_demand2.index\n",
    "\n",
    "\n",
    "##----hour bike return\n",
    "bike_h_return = bike_2017[['End date','End station number','Bike number']]\n",
    "bike_h_return['End date'] = bike_h_return['End date'].str.extract('(\\d{4}-\\d\\d-\\d\\d)\\s\\d\\d:\\d\\d:\\d\\d')\n",
    "\n",
    "bike_h_return2 = bike_h_return.groupby(['End date','End station number']).agg('count')\n",
    "bike_h_return2.rename({'Bike number':'Return'},axis=1,inplace=True)\n",
    "bike_h_return2.reset_index(level=0,inplace=True)\n",
    "bike_h_return2['gene'] = bike_h_return2.index\n",
    "bike_h_return2=bike_h_return2[['gene','Return']]\n",
    "bike_h_return2.set_index('gene',inplace=True)\n",
    "bike_h_return2_dict = bike_h_return2.to_dict()\n",
    "bike_h_demand2['Return'] = bike_h_demand2['index'].map(bike_h_return2_dict['Return'])\n",
    "del bike_h_demand2['index']\n",
    "\n",
    "\n",
    "bike_h_demand2['station'] = bike_h_demand2.index\n",
    "def get_second(df):\n",
    "    return df[1]\n",
    "bike_h_demand2['station'] = bike_h_demand2['station'].apply(get_second)\n",
    "bike_h_demand2['Total Slot']=bike_h_demand2['station'].map(bike_station_dict['SLOTS'])\n",
    "del bike_h_demand2['station']\n",
    "\n",
    "bike_h_demand2.index.names = ['Date','Station Number']\n",
    "bike_h_demand2.to_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/Clustering/Bike_daily_demand_return.csv')\n",
    "\n",
    "##by hour\n",
    "bike_hour_demand = bike_2017[['Start date','Start station number','Bike number']]\n",
    "bike_hour_demand['Start date'] = bike_hour_demand['Start date'].str.extract('(\\d{4}-\\d\\d-\\d\\d\\s\\d\\d):\\d\\d:\\d\\d')\n",
    "\n",
    "bike_hour_demand2 = bike_hour_demand.groupby(['Start date','Start station number']).agg('count')\n",
    "bike_hour_demand2.rename({'Bike number':'Demand'},axis=1,inplace=True)\n",
    "bike_hour_demand2['index'] = bike_hour_demand2.index\n",
    "##----hour bike return\n",
    "bike_hour_return = bike_2017[['End date','End station number','Bike number']]\n",
    "bike_hour_return['End date'] = bike_hour_return['End date'].str.extract('(\\d{4}-\\d\\d-\\d\\d\\s\\d\\d):\\d\\d:\\d\\d')\n",
    "\n",
    "bike_hour_return2 = bike_hour_return.groupby(['End date','End station number']).agg('count')\n",
    "bike_hour_return2.rename({'Bike number':'Return'},axis=1,inplace=True)\n",
    "bike_hour_return2['gene'] = bike_hour_return2.index\n",
    "bike_hour_return2=bike_hour_return2[['gene','Return']]\n",
    "bike_hour_return2.set_index('gene',inplace=True)\n",
    "bike_hour_return2_dict = bike_hour_return2.to_dict()\n",
    "bike_hour_demand2['Return'] = bike_hour_demand2['index'].map(bike_hour_return2_dict['Return'])\n",
    "del bike_hour_demand2['index']\n",
    "bike_hour_demand2['station'] = bike_hour_demand2.index\n",
    "\n",
    "bike_hour_demand2['station'] = bike_hour_demand2['station'].apply(get_second)\n",
    "bike_hour_demand2['Total Slot']=bike_hour_demand2['station'].map(bike_station_dict['SLOTS'])\n",
    "del bike_hour_demand2['station']\n",
    "bike_hour_demand2.index.names = ['Date','Station Number']\n",
    "bike_hour_demand2.to_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/Clustering/Bike_hourly_demand_return.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input holiday info into dataset\n",
    "weather_station = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/new dataset/weather_station(1).csv')\n",
    "weather_station['Date'] =  pd.to_datetime(weather_station['Date'])\n",
    "import numpy as no\n",
    "def return_holiday(df):\n",
    "    if df in not_holiday:\n",
    "        return 0\n",
    "    if df not in not_holiday:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.nan\n",
    "weather_station['holiday'] =   weather_station['Date'].apply(return_holiday)  \n",
    "##input ID, and each station's latitude and longitude into dataset    \n",
    "station_info = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/2017-capitalbikeshare-tripdata /Capital_Bike_Share_Locations (1).csv')\n",
    "station_info = station_info[['TERMINAL_NUMBER', 'LATITUDE', 'LONGITUDE']]\n",
    "station_lati = station_info[['TERMINAL_NUMBER', 'LATITUDE']]\n",
    "station_lati.set_index('TERMINAL_NUMBER',inplace=True)\n",
    "station_lati_dict = station_lati.to_dict()\n",
    "\n",
    "station_long = station_info[['TERMINAL_NUMBER', 'LONGITUDE']]\n",
    "station_long.set_index('TERMINAL_NUMBER',inplace=True)\n",
    "station_long_dict = station_long.to_dict()\n",
    "\n",
    "weather_station['Station Longitude'] = weather_station['Station Number'].map(station_long_dict['LONGITUDE'])\n",
    "weather_station['Station Latitude'] = weather_station['Station Number'].map(station_lati_dict['LATITUDE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input season info into dataset\n",
    "def return_season(df):\n",
    "    if df.month == 3 or df.month == 4 or df.month == 5:\n",
    "        return 1\n",
    "    if df.month == 6 or df.month == 7 or df.month == 8:\n",
    "        return 2\n",
    "    if df.month == 9 or df.month == 10 or df.month == 11:\n",
    "        return 3\n",
    "    if df.month == 12 or df.month == 1 or df.month == 2:\n",
    "        return 4\n",
    "\n",
    "weather_station['Season'] = weather_station['Date'].apply(return_season)\n",
    "\n",
    "##change the column order of the dataset\n",
    "del weather_station['Unnamed: 0']\n",
    "weather_station = weather_station[[ 'Date', 'Daily minimum temperature',\n",
    "       'Daily maximum temperature', 'Maximum steady wind',\n",
    "       'Total daily precipitation', 'Pressure', 'Description', 'holiday',\n",
    "        'Season','Station Number', 'Station Longitude', 'Station Latitude','Demand', 'Return',\n",
    "       'Total Slot', 'usage', 'balanced_positive' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DISTANCE ANALYSIS\n",
    "distance = pd.read_csv('/Users/reggieyang/Desktop/Python/Python Project/Dataset/part 2/distance.csv')\n",
    "\n",
    "distance.head()\n",
    "distance.info()\n",
    "distance['distance'] = distance['distance'].astype('str')\n",
    "def return_station(df):\n",
    "    if df=='nan':\n",
    "        return np.nan\n",
    "    else:\n",
    "        stations = []\n",
    "        strings = df.split(',')\n",
    "        for i in strings:\n",
    "            stations.append(str(distance.iloc[int(i)-1]['TERMINAL_NUMBER']))\n",
    "        st = ','.join(stations)\n",
    "        stations = []\n",
    "        return st\n",
    "distance['distance'] = distance['distance'].apply(return_station)\n",
    "distance.columns\n",
    "del distance['Unnamed: 0']\n",
    "distance.to_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/Clustering/station_distance_recommendation.csv')\n",
    "df2 = distance.loc[1,:]['distance']\n",
    "stations = []\n",
    "strings = df2.split(',')\n",
    "for i in strings:\n",
    "    stations.append(str(distance.iloc[int(i)-1].values[-1]))\n",
    "st = ','.join(stations)\n",
    "stations = []\n",
    "return st\n",
    "dis = distance['distance']\n",
    "\n",
    "''.join(['23','22','3333'])\n",
    "\n",
    "distance.iloc[233].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---unbalanced/balanced cut-off analysis\n",
    "\n",
    "daily_station = pd.read_csv('/Users/reggieyang/Desktop/Python/Data Mining Project/Clustering/Bike_daily_demand_return.csv')\n",
    "daily_station.head()\n",
    "\n",
    "daily_station['usage'] = (daily_station['Demand']-daily_station['Return'])/daily_station['Total Slot']\n",
    "daily_station['Return'] = daily_station['Return'].fillna(0)\n",
    "daily_station['Total Slot'] = daily_station['Total Slot'].fillna(daily_station['Total Slot'].median())\n",
    "daily_station['Station Number'].nunique()\n",
    "daily_station.isnull().sum()\n",
    "daily_station.describe()\n",
    "useage = daily_station['usage']\n",
    "return_unb = daily_station[daily_station['usage']<-1]\n",
    "return_unb['Station Number'].nunique()\n",
    "demand_unb = daily_station[daily_station['usage']>1]\n",
    "demand_unb['Station Number'].nunique()\n",
    "def balance(df):\n",
    "    if df<-1 or df>1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "daily_station['balanced_positive'] = daily_station['usage'].apply(balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##merge different datasets into a total datasets \n",
    "##preparation for analysis\n",
    "station = pd.read_csv(\"/Users/reggieyang/Desktop/Python/Data Mining Project/daily_station.csv\")\n",
    "weather = pd.read_csv(\"/Users/reggieyang/Desktop/Python/Data Mining Project/weather.csv\")\n",
    "\n",
    "weather = weather.drop(['Unnamed: 0','Snow depth', 'Icon','Maximum wind gust',], axis=1)\n",
    "weather['Maximum steady wind'] = weather['Maximum steady wind'].str.extract('(\\d+\\.*\\d*)')\n",
    "weather['Maximum steady wind'] = pd.to_numeric(weather['Maximum steady wind'])\n",
    "weather['Maximum steady wind'] = weather['Maximum steady wind'].fillna(weather['Maximum steady wind'].mean())\n",
    "\n",
    "weather['Total daily precipitation'] = weather['Total daily precipitation'].str.extract('(\\d+\\.*\\d*)')\n",
    "weather['Total daily precipitation'] = pd.to_numeric(weather['Total daily precipitation'])\n",
    "weather['Total daily precipitation'] = weather['Total daily precipitation'].fillna(weather['Total daily precipitation'].mean())\n",
    "\n",
    "weather['Pressure'] = weather['Pressure'].str.extract('(\\d+\\.*\\d*)')\n",
    "weather['Pressure'] = pd.to_numeric(weather['Pressure'])\n",
    "weather['Pressure'] = weather['Pressure'].fillna(weather['Pressure'].mean())\n",
    "\n",
    "weather['Daily minimum temperature'] = weather['Daily minimum temperature'].str.extract('(\\-*\\d+\\.*\\d*)')\n",
    "weather['Daily minimum temperature'] = pd.to_numeric(weather['Daily minimum temperature'])\n",
    "weather['Daily minimum temperature'] = weather['Daily minimum temperature'].fillna(weather['Daily minimum temperature'].mean())\n",
    "weather['Daily maximum temperature'] = weather['Daily maximum temperature'].str.extract('(\\-*\\d+\\.*\\d*)')\n",
    "weather['Daily maximum temperature'] = pd.to_numeric(weather['Daily maximum temperature'])\n",
    "weather['Daily maximum temperature'] = weather['Daily maximum temperature'].fillna(weather['Daily maximum temperature'].mean())\n",
    "\n",
    "weather['Date'] = weather['Date'].apply(lambda x: datetime.datetime.strptime(x, '%m/%d/%Y').strftime('%Y-%m-%d'))\n",
    "df = station.merge(weather, on ='Date', how = 'inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
